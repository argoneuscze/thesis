Sound as we know it can be defined as a physical wave travelling through air or another means. \cite{you_2010} It can be measured as change in air pressure surrounding an object. Once we have this electrical representation of the wave, we can convert it back and consequently play using speakers.

In the real world, these sound waves are generally composed of many different kinds of waves, with differing frequencies and amplitudes. The human ear can tell the difference between high (whistling) and low frequencies (drums), and knowledge of this will be useful later when we are discussing audio encoding.

- TODO image of audio signal -

\section{Important terms}
.. TODO ..
sampling
nyquist frequency/limit
quantization
transient
aliasing
spectral leakage
windowing

\section{Digital audio representation}
Most commonly, the amount of air pressure is sampled many times a second and after being processed this information is stored as a discrete-time signal using numerical representations - this is what's known as a \emph{digital audio signal}. This entire process is called \emph{digital audio encoding}.

By sampling the audio signal, we will potentially be losing out on some information, but given a high enough sampling rate, the result will be imperceptible to the human ear. For general purpose audio and music, the standard sampling rate is 48 kHz, alternatively 44.1 kHz from the compact disk era.

Once we have our digital signal, there are two distinct kinds of ways we can represent, or, encode it. Both of them have many different data models for encoding \cite{you_2010}, but in this work I am only going to focus on the most relevant ones.

- TODO what compromises are taken when encoding -

\subsection{Time domain representation}
In the time domain, the signal is simply represented as a function of time, where $t$ is the time and $x(t)$ is the raw amplitude, or air pressure, at that point. \cite{bosi_goldberg_2003}

This is the most straightforward representation since it directly correlates to how the signal is being captured in the first place. However, as we will see later, this format is not ideal for storing audio data with any sort of compression.

\subsubsection{PCM}
In the time domain, the most basic encoding we can use is PCM (Pulse Code Modulation). After sampling a signal at uniform intervals, the discrete values are quantized; that is, each range of values is assigned a symbol in (usually) binary code.

For example using 16-bit signed PCM, each sample will be represented as a 16-bit signed integer, or in the case of multiple channels, N 16-bit signed integers, where N is the amount of channels.

PCM serves as a good base for what we are going to talk about next - Frequency domain representation and encoding.

\subsection{Frequency domain representation}
While it's simple to understand and work with for the computer with samples in the form of a sequence of amplitudes, it's difficult to run any sort of meaningful analysis on such data. To better grasp the structure of the audio we're working with, it would be helpful to be able to decompose it into its basic building blocks, so to speak. And that's where frequency based representation comes in.

The goal here is to represent the signal as not a function of time, but rather a function of frequency $X(f)$. That is, instead of having a simple sequence of amplitudes, we will have information about the magnitude for each component from a set of frequency ranges. This description alone is generally more compact than the PCM representation \cite{bosi_goldberg_2003} on top of providing us with useful information about the signal, so it will serve as a good entry point to our compression schemes.

\subsubsection{Fourier transform}
Fourier transform is the first and arguably the most used tool for converting a signal from a function of time $x(t)$ into a function of frequency $X(f)$.

It is based on the \emph{Fourier series}, which is essentially a representation of a periodic function as the linear combination of sines and cosines. \cite{Shatkay:1995:FTP:864947} However, the main difference is that our function need not be periodic.

The Fourier transform of a continuous signal $s$ is defined as: \cite{Recoskie2014ConstrainedNM}

\begin{align}
S(\xi) = \int_{-\infty}^{\infty}s(t)e^{-2\pi it\xi}dt
\end{align}

If we inspect the formula, we can notice that Fourier transform essentially projects our signal into infinity - this wouldn't be a problem if it was a periodic signal, but sampled audio is generally constrained by time. To prevent spectral leakage, we must window the signal before processing it.

The output is a complex number, which provides us with the means to find the magnitude and phase offset for the sinusoid of each frequency $\xi$.

The Fourier transform can also be inverted, providing us with an easy way to obtain the original signal back from its frequency components. The inverse transform is defined as:

\begin{align}
s(t) = \int_{-\infty}^{\infty}S(\xi)e^{2\pi it\xi}d\xi
\end{align}

However, seeing as our samples are discretely sampled, we will need to modify our transform accordingly.

The discrete Fourier transform of a discrete signal $s_0, s_1, ..., s_{N-1}$ is: \cite{Recoskie2014ConstrainedNM}

\begin{align}
S_k = \sum_{n=0}^{N-1}s_ne^{-2\pi ikn/N}
\end{align}

And our inverse is:

\begin{align}
s_n = \frac1N \sum_{k=0}^{N-1}S_ke^{2\pi ikn/N}
\end{align}

The issue is, due to the nature of this process, if we run the Fourier transform on our whole signal, we will only be able to analyse it as a whole, e.g. we won't be able to tell which parts of for example a song are quiet or if there are any parts with very high frequencies - we lose our temporal data.

To alleviate this problem, we can run Fourier transform on smaller chunks of the signal, analyse them separately and later join them back into the original signal. That is the essence of the Short-time Fourier transform.

\subsubsection{Short-time Fourier transform}
When using Short-time Fourier transform, or STFT for short, we first split the signal into smaller segments of equal size and then run Fourier transform on those separately. As such, our output can be projected into two dimensions - namely a frequency spectrum as a function of time.

Doing it this way will let us see how the frequency components change over time instead of taking the spectrum of the entire signal.

As with regular Fourier transform, we'll need to window each segment of the signal, but there is a caveat. Since we have windowed segments, we may be losing some information at the edge of each segment leading to artifacts, and furthermore we may be losing information about transients. To solve this, we'll need to introduce overlapping windows - however, having an overlap will increase the amount of coefficients required.

The continuous version is defined as: \cite{Recoskie2014ConstrainedNM}

\begin{align}
S(\tau, \xi) = \int_{-\infty}^{\infty}s(t)w(t-\tau)e^{-2\pi it\xi}dt
\end{align}

where $w$ is the window function.

But again, as we have discrete samples, we will need to use a discrete short-time Fourier transform, specifically:

\begin{align}
S_{k, \xi} = \sum_{n=-\infty}^{\infty}s_nw_{n-k}e^{-2\pi i\xi n}
\end{align}

And similarly to the regular Fourier Transform, short-time Fourier Transform is also invertible. \cite{selesnick_2009}

STFT is most commonly used for audio analysis (TODO source) but in this case it will be used as a means for our NMF compression.

\subsubsection{Modified discrete cosine transform}
Modified discrete cosine transform, or MDCT for short, has become the dominant means of high-quality audio coding. \cite{wang_vilermo_2012_mdct}

It is what's known as a \emph{lapped transform}. This means that when transforming a block into its MDCT coefficients, the basis function overlaps the block's boundaries. \cite{Malvar:1992:SPL:531523} In practice, what this means is that while we have blocks with overlapping windows as in the short-time Fourier transform, the number of coefficients remains the same as without while retaining the relevant properties.

As the name suggests, MDCT is based on the Discrete cosine transform, namely \emph{DCT-IV}, where the main difference is the addition of lapping mentioned above.

What makes MDCT simpler to work with compared to Fourier transform is that not only do we not need more coefficients despite overlapping, they are also real numbers as opposed to complex numbers, lowering the amount of bytes necessary to store them.

It is a linear function $f: \mathbf{R}^{2N} \rightarrow \mathbf{R}^N$, defined as: \cite{Babu2013FastAE}

\begin{align}
X(k) = \sum_{n=0}^{N-1} x(n) \cos \left\lbrace \frac{(2n+1+ \frac{N}{2} )(2k+1)\pi }{2N} \right\rbrace
\end{align}

for $k = 0, 1, \ldots, \frac{N}{2}-1$.

It is assumed that $x(n)$ is already windowed by an appropriate windowing function $w$.

MDCT is also invertible, and its inversion is defined as:

\begin{align}
\bar{x}(n) &= \sum_{k=0}^{\frac{N}{2}-1} X(k) \cos \left\lbrace \frac{(2n+1+ \frac{N}{2} )(2k+1)\pi }{2N} \right\rbrace
\end{align}

for $n = 0, 1, \ldots, N-1$.

It's important to note that the inverted transformed sequence does not correspond to the original signal \cite{prince_1986_tdac_1}. To achieve perfect invertibility, we must add subsequent overlapping blocks of the inverted MDCT (IMDCT). This method is called \emph{time domain aliasing cancellation} \cite{prince_1986_tdac_2}, or TDAC for short. As the name suggests, it mainly helps remove artifacts on the boundaries between transform blocks.

\section{Psychoacoustics}
Apart from time-frequency representations being generally more compact, they also give us the ability to analyse, isolate or modify the frequency composition of a given signal. This comprises a large chunk of the audio compressing process.

The field of psychoacoustics studies sound perception - that is, how our ears work and how we perceive different kinds of sounds. There are many different characteristics to sound that need to be taken into account for a proper psychoacoustic analysis \cite{olson1967music}, split into several categories, namely:

\begin{description}
	\item[tonal] includes pitch, timbre, melody harmony
	\item[dynamic] based on loudness
	\item[temporal] involves time, duration, tempo and rhythm
	\item[qualitative] represents harmonic constitution of the tone
\end{description}

For music, it's important to balance these four qualities appropriately. For compression, the most important qualities for us in scope of this work are going to be tonal (pitch) and dynamic (loudness).

\subsection{Pitch}
Pitch is a characteristic that comes from a frequency. The difference between the two is that pitch is our subjective perception of the tone whereas a frequency is an objective measure. However, pitch is usually quantified as a frequency using Hertz as its unit.

The lower bound of human hearing is around 20 Hz whereas the upper bound is most commonly cited as 20 000 Hz, or 20 kHz. \cite{rosen1993hearing} In a laboratory environment, people have been found to hear as low as 12 Hz. As people age, our hearing gets progressively worse and a healthy adult younger than 40 years can generally perceive frequencies only up to 15 kHz. \cite{olson1967music}

Furthermore, people have different sensitivity to different frequencies.

.. TODO bark scale ..

\subsection{Loudness}

\subsection{Auditory masking}

.. TODO frequency masking / temporal masking ..
\cite{gelfand1990hearing}




